# Image_Captioning

## Overview
In this project I have a neural network architecture to automatically generate captions from images.  Pretty cool, huh!

The project is structured in a seris of Jupyter Notebooks that are designed to be completed in swquential order:
- 0_Dataset.ipynb
- 1_Preliminaries.ipynb
- 2_Training.ipynb
- 3_Inference.ipynb

It took about eight hours to train the network.  I used the (Microsoft COCO).[https://github.com/the-john/Image_Captioning/blob/master/Microsoft_COCO_Common_Objects_in_Context_1405_0312.pdf] data set.

I tried to emulate, as much as possible, these two white papers:
- 
